{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e79b233",
   "metadata": {},
   "source": [
    "# Demo: CityLearn\n",
    "\n",
    "In this demo, we will use [CityLearn](https://www.citylearn.net/index.html) ([Vazquez-Canteli et al., 2019](https://doi.org/10.1145/3360322.3360998)), a Gymnasium environment for control algorithms for building energy coordination and demand response in cities. \n",
    "\n",
    "This tutorial is based on the [Climate Change AI Citylearn Tutorial at ICLR 2023](https://www.climatechange.ai/tutorials?search=id%3Acitylearn)\n",
    "\n",
    "### What is CityLearn?\n",
    "\n",
    "\n",
    "<img src='https://www.citylearn.net/_images/dr.jpg' height=300></img> \\\n",
    "Source: [CityLearn](https://www.citylearn.net/)\n",
    "\n",
    "- Open source gymnasium enviroment for control algorithms for building and district energy systems\n",
    "- models buildings and districts as environments\n",
    "\n",
    "Buildings:\n",
    "- modeled as single thermal zone\n",
    "- up to five load types: space cooling, space heating domestic hot water (DHW) heating, electric equipment, electric vehicle (EV) loads\n",
    "- energy storage\n",
    "- Photovoltaic (PV) system\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/intelligent-environments-lab/CityLearn/blob/master/assets/images/environment.jpg?raw=true\"  width=\"1000\" alt=\"An overview of the heating, ventilation and air conditioning systems, energy storage systems, on-site electricity sources and grid interaction in buildings in the CityLearn environment.\"></img> \\\n",
    "Source: [Nweye et al., 2024](https://doi.org/10.48550/arXiv.2405.03848)\n",
    "\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "- Get familiar with the CityLearn framework\n",
    "- try different control algorithms\n",
    "  - Rule-based control (RBC)\n",
    "  - Q-Learning\n",
    "  - Deep reinforcement learning (Soft-Actor Critic SAC)\n",
    "- develop understanding which tools work for which task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d1707d",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We use the `citylearn_challenge_2022_phase_all` dataset from the [The CityLearn Challenge 2022](https://www.aicrowd.com/challenges/neurips-2022-citylearn-challenge), which is included in the CityLearn package. For more detailed information, look at the notebook from the [Climate Change AI Citylearn Tutorial at ICLR 2023](https://www.climatechange.ai/tutorials?search=id%3Acitylearn). The dataset consists of 17 single family homes, each with a 6.4 kWh capacity battery and PV modules. The data includes hourly energy consumption data from August 1, 2016 and July 31, 2017. All energy loads in a building are combined to a single load value. \n",
    "\n",
    "### Control strategy\n",
    "\n",
    "The goal is to implement a controller that manages energy storage and load shifting in a two-building district. The controller should **minimize electricity cost** by finding a strategy when to charge or discharge the battery. The *control action* $a$ is a value from the interval $[-1, 1]$, determining the proportion of the battery to be discharged ($a < 0$) or charged ($a > 0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd5d1b6",
   "metadata": {},
   "source": [
    "Install packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8ceedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install citylearn\n",
    "! pip install stable=baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31f3e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from citylearn.agents.base import (\n",
    "    BaselineAgent,\n",
    "    Agent as RandomAgent\n",
    ")\n",
    "from citylearn.agents.rbc import HourRBC\n",
    "from citylearn.agents.q_learning import TabularQLearning\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "from citylearn.data import DataSet\n",
    "from citylearn.reward_function import RewardFunction\n",
    "from citylearn.wrappers import (\n",
    "    NormalizedObservationWrapper,\n",
    "    StableBaselines3Wrapper,\n",
    "    TabularQLearningWrapper\n",
    ")\n",
    "\n",
    "from stable_baselines3 import SAC\n",
    "\n",
    "from citylearn_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86c73e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = 'citylearn_challenge_2022_phase_all'\n",
    "\n",
    "schema = DataSet().get_schema(DATASET_NAME)\n",
    "root_directory = schema['root_directory']\n",
    "\n",
    "# We use an environment with two buildiungs\n",
    "BUILDINGS = ['Building_1', 'Building_2']\n",
    "\n",
    "# We use the first week of september from the dataset\n",
    "SIMULATION_START_TIME_STEP = 745\n",
    "SIMULATION_END_TIME_STEP = SIMULATION_START_TIME_STEP + 24*7 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05049ea5",
   "metadata": {},
   "source": [
    "You can list all available datasets in CityLearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b5cd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataSet().get_dataset_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14257f45",
   "metadata": {},
   "source": [
    "Preview building data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acee558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, len(BUILDINGS), figsize=(5*len(BUILDINGS), 5), sharex=True, sharey='row')\n",
    "if len(BUILDINGS) == 1:\n",
    "    axs = axs.reshape(-1, 1)\n",
    "for i, building_name in enumerate(BUILDINGS):\n",
    "    filename = schema['buildings'][building_name]['energy_simulation']\n",
    "    filepath = os.path.join(root_directory, filename)\n",
    "    building_data = pd.read_csv(filepath).iloc[SIMULATION_START_TIME_STEP:SIMULATION_END_TIME_STEP+1]\n",
    "\n",
    "    filename = schema['buildings'][building_name]['pricing']\n",
    "    filepath = os.path.join(root_directory, filename)\n",
    "    pricing_data = pd.read_csv(filepath).iloc[SIMULATION_START_TIME_STEP:SIMULATION_END_TIME_STEP+1]\n",
    "\n",
    "    x = building_data.index\n",
    "    y1 = building_data['non_shiftable_load']\n",
    "    y2 = building_data['solar_generation']\n",
    "    y3 = pricing_data['electricity_pricing']\n",
    "    axs[0, i].plot(x, y1)\n",
    "    axs[1, i].plot(x, y2)\n",
    "    axs[2, i].plot(x, y3)\n",
    "    axs[2, i].set_xlabel('Time step')\n",
    "    if i == 0:\n",
    "        axs[0, i].set_ylabel('Non-shiftable load\\n[kWh]', fontsize=9)\n",
    "        axs[1, i].set_ylabel('Solar generation\\n[W/kW]', fontsize=9)\n",
    "        axs[2, i].set_ylabel('Electricity pricing\\n[W/kW]', fontsize=9)\n",
    "\n",
    "    axs[0, i].set_title(building_name)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b8a6c1",
   "metadata": {},
   "source": [
    "Weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0579eefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = schema['buildings'][BUILDINGS[0]]['weather']\n",
    "filepath = os.path.join(root_directory, filename)\n",
    "weather_data = pd.read_csv(filepath).iloc[SIMULATION_START_TIME_STEP:SIMULATION_END_TIME_STEP+1]\n",
    "\n",
    "columns = [\n",
    "    'outdoor_dry_bulb_temperature', 'outdoor_relative_humidity',\n",
    "    'diffuse_solar_irradiance', 'direct_solar_irradiance'\n",
    "]\n",
    "titles = [\n",
    "    'Outdoor dry-bulb\\ntemperature [C]', 'Relative humidity\\n[%]',\n",
    "    'Diffuse solar irradiance\\n[W/m2]', 'Direct solar irradiance\\n[W/m2]'\n",
    "]\n",
    "fig, axs = plt.subplots(4, 1, figsize=(5, 6), sharex=True)\n",
    "x = weather_data.index\n",
    "\n",
    "for ax, c, t in zip(fig.axes, columns, titles):\n",
    "    y = weather_data[c]\n",
    "    ax.plot(x, y)\n",
    "    # ax.set_xlabel('Time step')\n",
    "    ax.set_ylabel(t, fontsize=9)\n",
    "axs[-1].set_xlabel('Time step')\n",
    "fig.align_ylabels()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849e6b98",
   "metadata": {},
   "source": [
    "Carbon intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b286e36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = schema['buildings'][building_name]['carbon_intensity']\n",
    "filepath = os.path.join(root_directory, filename)\n",
    "carbon_intensity_data = pd.read_csv(filepath).iloc[SIMULATION_START_TIME_STEP:SIMULATION_END_TIME_STEP+1]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 2))\n",
    "x = carbon_intensity_data.index\n",
    "y = carbon_intensity_data['carbon_intensity']\n",
    "ax.plot(x, y)\n",
    "ax.set_xlabel('Time step')\n",
    "ax.set_ylabel('kg_CO2/kWh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebb71bc",
   "metadata": {},
   "source": [
    "### Evaluation metrics\n",
    "\n",
    "\n",
    "**Electricity cost**: sum of building-level imported electricity cost, $E_h^{\\textrm{building}} \\times T_h$ (\\$), where $T_h$ is the electricity rate at hour $h$.\n",
    "\n",
    "$$\n",
    "    \\textrm{cost} = \\sum_{h=0}^{n-1}{\\textrm{max} \\left (0,E_h^{\\textrm{building}} \\times T_h \\right )}\n",
    "$$\n",
    "\n",
    "**Carbon emissions**: sum of building-level carbon emissions (kg<sub>CO<sub>2</sub>e</sub>), $E_h^{\\textrm{building}} \\times O_h$, where $O_h$ is the carbon intensity (kg<sub>CO<sub>2</sub>e</sub>/kWh) at hour $h$.\n",
    "\n",
    "$$\n",
    "    \\textrm{carbon emissions} = \\sum_{h=0}^{n-1}{\\textrm{max} \\left (0,E_h^{\\textrm{building}} \\times O_h \\right )}\n",
    "$$\n",
    "\n",
    "**Average daily peak**: mean of the daily $E_h^{\\textrm{district}}$ peak where $d$ is the day index and $n$ is the total number of days.\n",
    "\n",
    "$$\n",
    "    \\textrm{average daily peak} = \\frac{\n",
    "        {\\sum}_{d=0}^{n - 1} {\\sum}_{h=0}^{23} {\\textrm{max} \\left (E_{24d + h}^{\\textrm{district}}, \\dots, E_{24d + 23}^{\\textrm{district}} \\right)}\n",
    "    }{n}\n",
    "$$\n",
    "\n",
    "**Ramping**: absolute difference of consecutive $E_h^{\\textrm{district}}$. It represents the smoothness of the district's load profile where low ramping means there is gradual increase in grid load even after self-generation becomes unavailable in the evening and early morning. High ramping means abrupt change in grid load that may lead to unscheduled strain on grid infrastructure and blackouts as a result of supply deficit.\n",
    "\n",
    "$$\n",
    "    \\textrm{ramping} = \\sum_{h=0}^{n-1}  \\lvert E_{h}^{\\textrm{district}} - E_{h - 1}^{\\textrm{district}} \\rvert\n",
    "$$\n",
    "\n",
    "**Load factor**: average ratio of monthly average and peak $E_{h}^{\\textrm{district}}$ where $m$ is the month index, $d$ is the number of days in a month and $n$ is the number of months. Load factor represents the efficiency of electricity consumption and is bounded between 0 (very inefficient) and 1 (highly efficient) thus, the goal is to maximize the load factor or in the same fashion as the other KPIs, minimize (1 - load factor).\n",
    "\n",
    "$$\n",
    "    \\textrm{1 - load factor}  = \\Big(\n",
    "        \\sum_{m=0}^{n - 1} 1 - \\frac{\n",
    "            \\left (\n",
    "                \\sum_{h=0}^{d - 1} E_{d \\cdot m + h}^{\\textrm{district}}\n",
    "            \\right ) \\div d\n",
    "        }{\n",
    "            \\textrm{max} \\left (E_{d \\cdot m}^{\\textrm{district}}, \\dots, E_{d \\cdot m + d - 1}^{\\textrm{district}} \\right )\n",
    "    }\\Big) \\div n\n",
    "$$\n",
    "\n",
    "The KPIs are reported as normalized values with respect to the baseline outcome where the baseline outcome is when buildings are not equipped with batteries i.e., no control. Thus a KPI less than 1.0 is preferred to make a case for including the battery or an advanced control approach.\n",
    "\n",
    "$$\n",
    "    \\textrm{KPI} = \\frac{{\\textrm{KPI}_{control}}}{\\textrm{KPI}_{baseline (no\\ battery)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23055613",
   "metadata": {},
   "source": [
    "Initialize CityLearn Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5714393",
   "metadata": {},
   "outputs": [],
   "source": [
    "CENTRAL_AGENT = True\n",
    "ACTIVE_OBSERVATIONS = ['hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e9fbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CityLearnEnv(\n",
    "    DATASET_NAME,\n",
    "    central_agent=CENTRAL_AGENT,\n",
    "    buildings=BUILDINGS,\n",
    "    active_observations=ACTIVE_OBSERVATIONS,\n",
    "    simulation_start_time_step=SIMULATION_START_TIME_STEP,\n",
    "    simulation_end_time_step=SIMULATION_END_TIME_STEP,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831d5391",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Current time step:', env.time_step)\n",
    "print('environment number of time steps:', env.time_steps)\n",
    "print('environment uses central agent:', env.central_agent)\n",
    "print('Number of buildings:', len(env.buildings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e065ae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# electrical storage\n",
    "print('Electrical storage capacity:', {\n",
    "    b.name: b.electrical_storage.capacity for b in env.buildings\n",
    "})\n",
    "print('Electrical storage nominal power:', {\n",
    "    b.name: b.electrical_storage.nominal_power for b in env.buildings\n",
    "})\n",
    "print('Electrical storage loss_coefficient:', {\n",
    "    b.name: b.electrical_storage.loss_coefficient for b in env.buildings\n",
    "})\n",
    "print('Electrical storage soc:', {\n",
    "    b.name: b.electrical_storage.soc[b.time_step] for b in env.buildings\n",
    "})\n",
    "print('Electrical storage efficiency:', {\n",
    "    b.name: b.electrical_storage.efficiency for b in env.buildings\n",
    "})\n",
    "print('Electrical storage electricity consumption:', {\n",
    "    b.name: b.electrical_storage.electricity_consumption[b.time_step]\n",
    "    for b in env.buildings\n",
    "})\n",
    "print('Electrical storage capacity loss coefficient:', {\n",
    "    b.name: b.electrical_storage.capacity_loss_coefficient for b in env.buildings\n",
    "})\n",
    "print()\n",
    "# pv\n",
    "print('PV nominal power:', {\n",
    "    b.name: b.pv.nominal_power for b in env.buildings\n",
    "})\n",
    "print()\n",
    "# active observations\n",
    "print('Active observations:', {b.name: b.active_observations for b in env.buildings})\n",
    "# active actions\n",
    "print('Active actions:', {b.name: b.active_actions for b in env.buildings})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50784ad2",
   "metadata": {},
   "source": [
    "### Test 1: Baseline controller (no control)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e95491",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_env = CityLearnEnv(\n",
    "    DATASET_NAME,\n",
    "    central_agent=CENTRAL_AGENT,\n",
    "    buildings=BUILDINGS,\n",
    "    active_observations=ACTIVE_OBSERVATIONS,\n",
    "    simulation_start_time_step=SIMULATION_START_TIME_STEP,\n",
    "    simulation_end_time_step=SIMULATION_END_TIME_STEP,\n",
    ")\n",
    "\n",
    "baseline_model = BaselineAgent(baseline_env)\n",
    "\n",
    "# always start by reseting the environment\n",
    "observations, _ = baseline_env.reset()\n",
    "\n",
    "# step through the environment until terminal\n",
    "# state is reached i.e., the control episode ends\n",
    "while not baseline_env.terminated:\n",
    "    # select actions from the model\n",
    "    actions = baseline_model.predict(observations)\n",
    "\n",
    "    # apply selected actions to the environment\n",
    "    observations, _, _, _, _ = baseline_env.step(actions)\n",
    "\n",
    "plot_simulation_summary({\n",
    "    'Baseline': baseline_env,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d0db03",
   "metadata": {},
   "source": [
    "### Test 2: Random control strategy\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7de085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize your environment\n",
    "random_env = CityLearnEnv(\n",
    "    DATASET_NAME,\n",
    "    central_agent=CENTRAL_AGENT,\n",
    "    buildings=BUILDINGS,\n",
    "    active_observations=ACTIVE_OBSERVATIONS,\n",
    "    simulation_start_time_step=SIMULATION_START_TIME_STEP,\n",
    "    simulation_end_time_step=SIMULATION_END_TIME_STEP,\n",
    ")\n",
    "\n",
    "# initialize your agent\n",
    "random_model = RandomAgent(\n",
    "    random_env\n",
    ")\n",
    "\n",
    "# reset your environment\n",
    "observations, _ = random_env.reset()\n",
    "\n",
    "# step through your environment\n",
    "while not random_env.terminated:\n",
    "    # select actions\n",
    "    actions = random_model.predict(observations)\n",
    "\n",
    "    # apply actions to environment step function\n",
    "    observations, _, _, _, _ = random_env.step(actions)\n",
    "\n",
    "# display simulation summary figures\n",
    "plot_simulation_summary({\n",
    "    'Baseline': baseline_env,\n",
    "    'Random': random_env,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fe2e01",
   "metadata": {},
   "source": [
    "### Test 3: Rule-based control (RBC)\n",
    "---\n",
    "\n",
    "Our next step is to implement a rule-based control (RBC) algorithm. We will implement a simple time based controller: each hour of the day, the controller performs a fixed action to charge/discharge the battery. Here is an example of a possible strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc31a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define action map\n",
    "action_map = {\n",
    "    1: 1/12, # Rule for 1 AM\n",
    "    2: 1/12, # Rule for 2 AM\n",
    "    3: 1/12, # Rule for 3 AM\n",
    "    4: 1/12, # Rule for 4 AM\n",
    "    5: 1/12, # Rule for 5 AM\n",
    "    6: 1/12, # Rule for 6 AM\n",
    "    7: 1/12, # Rule for 7 AM\n",
    "    8: 1/12, # Rule for 8 AM\n",
    "    9: 1/12, # Rule for 9 AM\n",
    "    10: 1/12, # Rule for 10 AM\n",
    "    11: 1/12, # Rule for 11 AM\n",
    "    12: 1/12, # Rule for 12 PM\n",
    "    13: -1/12, # Rule for 1 PM\n",
    "    14: -1/12, # Rule for 2 PM\n",
    "    15: -1/12, # Rule for 3 PM\n",
    "    16: -1/12, # Rule for 4 PM\n",
    "    17: -1/12, # Rule for 5 PM\n",
    "    18: -1/12, # Rule for 6 PM\n",
    "    19: -1/12, # Rule for 7 PM\n",
    "    20: -1/12, # Rule for 8 PM\n",
    "    21: -1/12, # Rule for 9 PM\n",
    "    22: -1/12, # Rule for 10 PM\n",
    "    23: -1/12, # Rule for 11 PM\n",
    "    24: -1/12, # Rule for 12 AM\n",
    "}\n",
    "\n",
    "# run inference\n",
    "rbc_env = CityLearnEnv(\n",
    "    DATASET_NAME,\n",
    "    central_agent=CENTRAL_AGENT,\n",
    "    buildings=BUILDINGS,\n",
    "    active_observations=ACTIVE_OBSERVATIONS,\n",
    "    simulation_start_time_step=SIMULATION_START_TIME_STEP,\n",
    "    simulation_end_time_step=SIMULATION_END_TIME_STEP,\n",
    ")\n",
    "rbc_model = HourRBC(rbc_env, action_map=action_map)\n",
    "observations, _ = rbc_env.reset()\n",
    "\n",
    "while not rbc_env.terminated:\n",
    "    actions = rbc_model.predict(observations)\n",
    "    observations, _, _, _, _ = rbc_env.step(actions)\n",
    "\n",
    "# display simulation summary\n",
    "plot_simulation_summary({\n",
    "    'Baseline': baseline_env,\n",
    "    'Random': random_env,\n",
    "    'RBC': rbc_env,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31020f9a",
   "metadata": {},
   "source": [
    "### Exercise: design RBC controller\n",
    "---\n",
    "Our RBC is not very good yet - try to find a hourly control strategy that improves the electrical cost and emissions compared to the baseline! Look at the daily load profiles below to decide when to charge or discharge the battery and think about when energy from the PV is available (look at the weather data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8379cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Building-level daily-average load profiles:')\n",
    "plot_building_load_profiles({'Baseline': baseline_env}, daily_average=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd05f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define action map\n",
    "action_map = {\n",
    "    1: 0.0,\n",
    "    2: 0.0,\n",
    "    3: 0.0,\n",
    "    4: 0.0,\n",
    "    5: 0.0,\n",
    "    6: 0.0,\n",
    "    7: 0.0,\n",
    "    8: 0.0,\n",
    "    9: 0.0,\n",
    "    10: 0.0,\n",
    "    11: 0.0,\n",
    "    12: 0.0,\n",
    "    13: 0.0,\n",
    "    14: 0.0,\n",
    "    15: 0.0,\n",
    "    16: 0.0,\n",
    "    17: 0.0,\n",
    "    18: 0.0,\n",
    "    19: 0.0,\n",
    "    20: 0.0,\n",
    "    21: 0.0,\n",
    "    22: 0.0,\n",
    "    23: 0.0,\n",
    "    24: 0.0,\n",
    "}\n",
    "\n",
    "# run inference\n",
    "rbc_env = CityLearnEnv(\n",
    "    DATASET_NAME,\n",
    "    central_agent=CENTRAL_AGENT,\n",
    "    buildings=BUILDINGS,\n",
    "    active_observations=ACTIVE_OBSERVATIONS,\n",
    "    simulation_start_time_step=SIMULATION_START_TIME_STEP,\n",
    "    simulation_end_time_step=SIMULATION_END_TIME_STEP,\n",
    ")\n",
    "rbc_model = HourRBC(rbc_env, action_map=action_map)\n",
    "observations, _ = rbc_env.reset()\n",
    "\n",
    "while not rbc_env.terminated:\n",
    "    actions = rbc_model.predict(observations)\n",
    "    observations, _, _, _, _ = rbc_env.step(actions)\n",
    "\n",
    "# display simulation summary\n",
    "plot_simulation_summary({\n",
    "    'Baseline': baseline_env,\n",
    "    'Random': random_env,\n",
    "    'RBC': rbc_env\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e3a5c8",
   "metadata": {},
   "source": [
    "### Test 4: RL controller (Q-learning)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd5852d",
   "metadata": {},
   "source": [
    "#### Reward function\n",
    "\n",
    "First, let's define our reward function: our goal is to minimize the electricity cost $C$ by using storing energy generated with the PV modules and using it when demand and costs are high. We assign a penalty $p$ for each building $i$ when (i) the battery is charged but not used or (ii) the battery is not charged but PV energy is fed into the grid.\n",
    "\n",
    "\n",
    "$$\n",
    "    r = \\sum_{i=0}^n \\Big(p_i \\times |C_i|\\Big)\n",
    "$$\n",
    "\n",
    "$$\n",
    "    p_i = -\\left(1 + \\textrm{sign}(C_i) \\times \\textrm{SoC}^{\\textrm{battery}}_i\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "When developing a RL solution, the design of the reward function is extremely important and has a great impact on how and what the agent learns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37acf89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnergyCostReward(RewardFunction):\n",
    "    def __init__(self, env_metadata: dict[str, Any]):\n",
    "        r\"\"\"Initialize CustomReward.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        env_metadata: dict[str, Any]:\n",
    "            General static information about the environment.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(env_metadata)\n",
    "\n",
    "    def calculate(\n",
    "        self, observations: list[dict[str, int | float]]\n",
    "    ) -> list[float]:\n",
    "        r\"\"\"Returns reward for most recent action.\n",
    "\n",
    "        The reward is designed to minimize electricity cost.\n",
    "        It is calculated for each building, i and summed to provide the agent\n",
    "        with a reward that is representative of all n buildings.\n",
    "        It encourages net-zero energy use by penalizing grid load satisfaction\n",
    "        when there is energy in the battery as well as penalizing\n",
    "        net export when the battery is not fully charged through the penalty\n",
    "        term. There is neither penalty nor reward when the battery\n",
    "        is fully charged during net export to the grid. Whereas, when the\n",
    "        battery is charged to capacity and there is net import from the\n",
    "        grid the penalty is maximized.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observations: list[dict[str, int | float]]\n",
    "            List of all building observations at current\n",
    "            :py:attr:`citylearn.citylearn.CityLearnEnv.time_step`\n",
    "            that are got from calling\n",
    "            :py:meth:`citylearn.building.Building.observations`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        reward: list[float]\n",
    "            Reward for transition to current timestep.\n",
    "        \"\"\"\n",
    "\n",
    "        reward_list = []\n",
    "\n",
    "        for o, m in zip(observations, self.env_metadata['buildings']):\n",
    "            cost = o['net_electricity_consumption']*o['electricity_pricing']\n",
    "            battery_soc = o['electrical_storage_soc']\n",
    "            penalty = -(1.0 + np.sign(cost)*battery_soc)\n",
    "            reward = penalty*abs(cost)\n",
    "            reward_list.append(reward)\n",
    "\n",
    "        reward = [sum(reward_list)]\n",
    "\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9ae5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tql_env = CityLearnEnv(\n",
    "    DATASET_NAME,\n",
    "    central_agent=CENTRAL_AGENT,\n",
    "    buildings=BUILDINGS,\n",
    "    active_observations=ACTIVE_OBSERVATIONS, \n",
    "    simulation_start_time_step=SIMULATION_START_TIME_STEP,\n",
    "    simulation_end_time_step=SIMULATION_END_TIME_STEP,\n",
    "    reward_function=EnergyCostReward  # add our new reward function\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96ab69c",
   "metadata": {},
   "source": [
    "#### Discretize action space and state space\n",
    "\n",
    "The tabular Q-Learning algorithm does only work for *discrete* action and state spaces - i.e. when there is a finite set of actions (move up/down, for example) and states. In our case, both spaces are *continuous*, hence we have to discretize the action and state space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d55f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define active observations and actions and their bin sizes\n",
    "observation_bins = {'hour': 24}\n",
    "action_bins = {'electrical_storage': 12}\n",
    "\n",
    "# initialize list of bin sizes where each building\n",
    "# has a dictionary in the list definining its bin sizes\n",
    "observation_bin_sizes = []\n",
    "action_bin_sizes = []\n",
    "\n",
    "for b in tql_env.buildings:\n",
    "    # add a bin size definition for the buildings\n",
    "    observation_bin_sizes.append(observation_bins)\n",
    "    action_bin_sizes.append(action_bins)\n",
    "\n",
    "\n",
    "# debug error in current CityLearn version\n",
    "class TabularQLearningWrapperDebug(TabularQLearningWrapper):\n",
    "    def __init__(self, env, observation_bin_sizes = None, action_bin_sizes = None, default_observation_bin_size = None, default_action_bin_size = None):\n",
    "        super().__init__(env, observation_bin_sizes, action_bin_sizes, default_observation_bin_size, default_action_bin_size)\n",
    "        self.observation_names = env.observation_names\n",
    "\n",
    "\n",
    "observation_names = tql_env.observation_names\n",
    "tql_env = TabularQLearningWrapperDebug(\n",
    "    tql_env,\n",
    "    observation_bin_sizes=observation_bin_sizes,\n",
    "    action_bin_sizes=action_bin_sizes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520c7864",
   "metadata": {},
   "source": [
    "Run training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf50e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- CALCULATE NUMBER OF TRAINING EPISODES -----------------\n",
    "m = tql_env.observation_space[0].n\n",
    "n = tql_env.action_space[0].n\n",
    "\n",
    "tql_episodes = 100\n",
    "\n",
    "print('Q-Table dimension:', (m, n))\n",
    "print('Number of episodes to train:', tql_episodes)\n",
    "\n",
    "# ----------------------- SET MODEL HYPERPARAMETERS -----------------------\n",
    "tql_kwargs = {\n",
    "    'epsilon': 1.0,\n",
    "    'minimum_epsilon': 0.01,\n",
    "    'epsilon_decay': 0.0001,\n",
    "    'learning_rate': 0.005,\n",
    "    'discount_factor': 0.99,\n",
    "}\n",
    "\n",
    "# ----------------------- INITIALIZE AND TRAIN MODEL ----------------------\n",
    "tql_model = TabularQLearning(\n",
    "    env=tql_env,\n",
    "    random_seed=None,\n",
    "    **tql_kwargs\n",
    ")\n",
    "\n",
    "print(tql_model.q[0].shape)\n",
    "\n",
    "for i in tqdm(range(tql_episodes)):\n",
    "    _ = tql_model.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bc1461",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations, _ = tql_env.reset()\n",
    "\n",
    "while not tql_env.unwrapped.terminated:\n",
    "    actions = tql_model.predict(observations, deterministic=True)\n",
    "    observations, _, _, _, _ = tql_env.step(actions)\n",
    "\n",
    "# plot summary and compare with other control results\n",
    "plot_simulation_summary({\n",
    "    'Baseline': baseline_env,\n",
    "    'Random': random_env,\n",
    "    'RBC': rbc_env,\n",
    "    'TQL': tql_env\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13885f6e",
   "metadata": {},
   "source": [
    "#### Why the bad performance?\n",
    "\n",
    " As stated before, tabular Q-learning is only applicable to discrete state and action spaces. The number of possible state-action pairs increases exponentially with the number of states and actions - traditional Q-learning is inefficient in exploring these large spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb49da77",
   "metadata": {},
   "source": [
    "### Test 5: Deep Reinforcement Learning Controller (SAC)\n",
    "---\n",
    "\n",
    "To overcome the curse of dimensionality, we use a neural network based function approximatior RL-algorithm called *Soft-Actor Critic* (SAC). It consists of two neural networks: the actor network learns which actions to take in a state, the critic network then evaluates those actions by learning corresponding q values from the state-action pairs. For detailed information, see the [original paper](https://proceedings.mlr.press/v80/haarnoja18b) or [here](https://spinningup.openai.com/en/latest/algorithms/sac.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ab27c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sac_env = CityLearnEnv(\n",
    "    DATASET_NAME,\n",
    "    central_agent=CENTRAL_AGENT,\n",
    "    buildings=BUILDINGS,\n",
    "    active_observations=ACTIVE_OBSERVATIONS,\n",
    "    simulation_start_time_step=SIMULATION_START_TIME_STEP,\n",
    "    simulation_end_time_step=SIMULATION_END_TIME_STEP,\n",
    "    reward_function=EnergyCostReward\n",
    ")\n",
    "\n",
    "sac_env = NormalizedObservationWrapper(sac_env)\n",
    "sac_env = StableBaselines3Wrapper(sac_env)\n",
    "sac_model = SAC(policy='MlpPolicy', env=sac_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff5a88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- CALCULATE NUMBER OF TRAINING EPISODES -----------------\n",
    "sac_episodes = 25\n",
    "sac_episode_timesteps = sac_env.unwrapped.time_steps - 1\n",
    "sac_total_timesteps = sac_episodes*sac_episode_timesteps\n",
    "\n",
    "# ------------------------------- TRAIN MODEL -----------------------------\n",
    "for i in tqdm(range(sac_episodes)):\n",
    "    sac_model = sac_model.learn(\n",
    "        total_timesteps=sac_episode_timesteps,\n",
    "        reset_num_timesteps=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e726fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations, _ = sac_env.reset()\n",
    "sac_actions_list = []\n",
    "\n",
    "while not sac_env.unwrapped.terminated:\n",
    "    actions, _ = sac_model.predict(observations, deterministic=True)\n",
    "    observations, _, _, _, _ = sac_env.step(actions)\n",
    "    sac_actions_list.append(actions)\n",
    "\n",
    "# plot summary and compare with other control results\n",
    "plot_simulation_summary({\n",
    "    'Baseline': baseline_env,\n",
    "    'Random': random_env,\n",
    "    'RBC': rbc_env,\n",
    "    'TQL': tql_env,\n",
    "    'SAC-1': sac_env\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134ed905",
   "metadata": {},
   "source": [
    "### Exercise: Improve SAC controller\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf064f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Set SAC observation space ===\n",
    "\n",
    "active_obs = [\n",
    "    'hour',\n",
    "    'day_type',\n",
    "    'solar_generation',\n",
    "    'net_electricity_consumption',\n",
    "    'electrical_storage_soc'\n",
    "    ]\n",
    "\n",
    "sac2_env = CityLearnEnv(\n",
    "    DATASET_NAME,\n",
    "    central_agent=CENTRAL_AGENT,\n",
    "    buildings=BUILDINGS,\n",
    "    active_observations=active_obs,\n",
    "    simulation_start_time_step=SIMULATION_START_TIME_STEP,\n",
    "    simulation_end_time_step=SIMULATION_END_TIME_STEP,\n",
    "    reward_function=EnergyCostReward\n",
    ")\n",
    "\n",
    "# === Set SAC hyperparameters ===\n",
    "your_agent_kwargs = {\n",
    "    'learning_rate': 0.0003,\n",
    "    'buffer_size': 1000000,\n",
    "    'learning_starts': 100,\n",
    "    'batch_size': 256,\n",
    "    'tau': 0.005,\n",
    "    'gamma': 0.99,\n",
    "    'train_freq': 1,\n",
    "}\n",
    "\n",
    "sac2_env = NormalizedObservationWrapper(sac2_env)\n",
    "sac2_env = StableBaselines3Wrapper(sac2_env)\n",
    "sac2_model = SAC(policy='MlpPolicy', env=sac2_env)\n",
    "\n",
    "sac_episodes = 25\n",
    "sac_episode_timesteps = sac2_env.unwrapped.time_steps - 1\n",
    "sac_total_timesteps = sac_episodes*sac_episode_timesteps\n",
    "\n",
    "# ------------------------------- TRAIN MODEL -----------------------------\n",
    "for i in tqdm(range(sac_episodes)):\n",
    "    sac2_model = sac2_model.learn(\n",
    "        total_timesteps=sac_episode_timesteps,\n",
    "        reset_num_timesteps=False,\n",
    "    )\n",
    "\n",
    "observations, _ = sac2_env.reset()\n",
    "sac_actions_list = []\n",
    "\n",
    "while not sac2_env.unwrapped.terminated:\n",
    "    actions, _ = sac2_model.predict(observations, deterministic=True)\n",
    "    observations, _, _, _, _ = sac2_env.step(actions)\n",
    "    sac_actions_list.append(actions)\n",
    "\n",
    "\n",
    "plot_simulation_summary({\n",
    "    'Baseline': baseline_env,\n",
    "    'SAC-1': sac_env,\n",
    "    'SAC-2': sac2_env.unwrapped\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
