{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8630f0b",
   "metadata": {},
   "source": [
    "# Demo: Reinforcement Learning Gym\n",
    "\n",
    "\n",
    "We will learn how to use [*Gymnasium*](https://gymnasium.farama.org/), an open framework to evaluate RL algorithms on different environments. We will train a simple Q-learning agent for the [Taxi environment](https://gymnasium.farama.org/environments/toy_text/taxi/). You can find more tutorials and infos how to [train a RL agent](https://gymnasium.farama.org/introduction/train_agent/) on the Gymnasium website.\n",
    "\n",
    "## Taxi environment\n",
    "\n",
    "<img src='./img/taxi_env.png' height=300></img>\n",
    "\n",
    "The RL agent has to navigate a grid world, pick up a passenger at one of 4 possible locations and drop the passenger off at one of 4 possible locations.\n",
    "\n",
    "**Action space (6):**\n",
    "  - Move up, down, left, right\n",
    "  - Pickup passenger\n",
    "  - Drop-off passenger\n",
    "  \n",
    "**States (500):**\n",
    "  - Taxi position (25)\n",
    "  - Passenger location (5)\n",
    "  - Drop off location (4)\n",
    "  \n",
    "**Rewards:**\n",
    "  - -1 per step (if no other rewards)\n",
    "  - +20 for delivering passenger\n",
    "  - -10 for wrong pickup/drop-off actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422eef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from matplotlib.animation import ArtistAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "from taxi_gym_utils import *\n",
    "\n",
    "matplotlib.rcParams['animation.embed_limit'] = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812c0059",
   "metadata": {},
   "source": [
    "### Taxi agent\n",
    "\n",
    "We first have to create the taxi agent. We will use the famous Q-Learning algorithm:\n",
    "\n",
    "\n",
    "$$\n",
    "Q(s, a) = Q(s, a) + \\alpha [r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)]\n",
    "$$\n",
    "\n",
    "$Q(s, a)$ &emsp; Q-value \\\n",
    "$a$ &emsp; action \\\n",
    "$s$ &emsp; state \\\n",
    "$r$ &emsp; reward \\\n",
    "$\\alpha \\in [0, 1]$ &emsp; learning rate \\\n",
    "$\\gamma$ &emsp; discount factor \\\n",
    "$\\max_{a'} Q(s', a')$ &emsp; maximum Q-value for all actions $a'$ in the next state $s'$\n",
    "\n",
    "**Goal**: find the *optimal policy* $\\pi^*$ that, in every state $s$, takes the action $a$ that *maximizes* $Q$.\n",
    "\n",
    "To learn the optimal policy, the RL agent has to *exploit* good actions while also *exploring* the environment by taking actions with unknow outcome. This is called the *exploration-exploitation dilemma*. In the algorithm, the willingness of the RL agent to explore new states is controlled by the constant $\\epsilon$: it defines the probability that the agent takes a random action. The action with highest expected return is taken with probability $1 - \\epsilon$ (exploitation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1890035",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaxiAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        learning_rate: float,\n",
    "        initial_epsilon: float,\n",
    "        epsilon_decay: float,\n",
    "        final_epsilon: float,\n",
    "        discount_factor: float = 0.95,\n",
    "    ):\n",
    "        \"\"\"Initialize a Reinforcement Learning agent with an empty dictionary\n",
    "        of state-action values (q_values), a learning rate and an epsilon.\n",
    "\n",
    "        Args:\n",
    "            env: The training environment\n",
    "            learning_rate: The learning rate\n",
    "            initial_epsilon: The initial epsilon value\n",
    "            epsilon_decay: The decay for epsilon\n",
    "            final_epsilon: The final epsilon value\n",
    "            discount_factor: The discount factor for computing the Q-value\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "\n",
    "        self.training_error = []\n",
    "\n",
    "    def get_action(self, obs: tuple[int, int, bool]) -> int:\n",
    "        \"\"\"\n",
    "        Returns the best action with probability (1 - epsilon)\n",
    "        otherwise a random action with probability epsilon to ensure exploration.\n",
    "        \"\"\"\n",
    "        # with probability epsilon return a random action to explore the environment\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        # with probability (1 - epsilon) act greedily (exploit)\n",
    "        else:\n",
    "            return int(np.argmax(self.q_values[obs]))\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        obs: tuple[int, int, bool],\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        terminated: bool,\n",
    "        next_obs: tuple[int, int, bool],\n",
    "    ):\n",
    "        \"\"\"Updates the Q-value of an action.\"\"\"\n",
    "        future_q_value = (not terminated) * np.max(self.q_values[next_obs])\n",
    "        temporal_difference = (\n",
    "            reward + self.discount_factor * future_q_value - self.q_values[obs][action]\n",
    "        )\n",
    "\n",
    "        self.q_values[obs][action] = (\n",
    "            self.q_values[obs][action] + self.lr * temporal_difference\n",
    "        )\n",
    "        self.training_error.append(temporal_difference)\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a423ca97",
   "metadata": {},
   "source": [
    "Initialize environment and agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5397cb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === hyperparameters ===\n",
    "learning_rate = 0.01\n",
    "n_episodes = 10000\n",
    "max_steps = 200\n",
    "start_epsilon = 1.0\n",
    "epsilon_decay = start_epsilon / (n_episodes / 2)  # reduce the exploration over time\n",
    "final_epsilon = 0.1\n",
    "# =======================\n",
    "\n",
    "# initialize environment\n",
    "env = gym.make(\"Taxi-v3\", max_episode_steps=max_steps, render_mode=\"rgb_array\")\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=n_episodes)\n",
    "env.reset()\n",
    "\n",
    "# create agent\n",
    "agent = TaxiAgent(\n",
    "    env=env,\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544356e8",
   "metadata": {},
   "source": [
    "Run training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b455780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset() # reset environment\n",
    "frames_first_ep = []\n",
    "frames_last_ep = []\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    # play one episode\n",
    "    while not done:\n",
    "        action = agent.get_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        if episode == 0:\n",
    "            frame = env.env.render()\n",
    "            frames_first_ep.append(frame)\n",
    "        elif episode == n_episodes - 1:\n",
    "            frame = env.env.render()\n",
    "            frames_last_ep.append(frame)\n",
    "\n",
    "        # update the agent\n",
    "        agent.update(obs, action, reward, terminated, next_obs)\n",
    "\n",
    "        # stop if goal is achieved or max_steps is reached\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "\n",
    "    agent.decay_epsilon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e6e4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_statistics(env, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5a5224",
   "metadata": {},
   "source": [
    "Performance without training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422b3d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_frameon(False)\n",
    "ax.set_axis_off()\n",
    "fig.tight_layout()\n",
    "ims = [[ax.imshow(f)] for f in frames_first_ep]\n",
    "ani = ArtistAnimation(fig, ims, interval=100, blit=True)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043d0401",
   "metadata": {},
   "source": [
    "Performance after 10000 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062206e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_frameon(False)\n",
    "ax.set_axis_off()\n",
    "fig.tight_layout()\n",
    "ims = [[ax.imshow(f)] for f in frames_last_ep]\n",
    "ani = ArtistAnimation(fig, ims, interval=100, blit=True)\n",
    "HTML(ani.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
